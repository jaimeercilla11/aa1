{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1bb47e",
   "metadata": {},
   "source": [
    "- Carga el archivo iris.csv.\n",
    "- Codifica la columna class como variable numérica (LabelEncoder).\n",
    "- Normaliza los datos con MinMaxScaler.\n",
    "- Separa en entrenamiento (80%) y prueba (20%) con semilla 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"iris.csv\", sep=\",\")\n",
    "X = df.drop(\"class\", axis=1)\n",
    "y = LabelEncoder().fit_transform(df[\"class\"])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "print(\"Datos cargados y preparados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944d9c8d",
   "metadata": {},
   "source": [
    "Entrena los siguientes modelos y compara su exactitud:\n",
    "\n",
    "- Regresión logística\n",
    "- Perceptrón\n",
    "- SVM lineal\n",
    "- KNN (k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "modelos ={\n",
    "    \"Regresion logistica\": LogisticRegression(max_iter=1000),\n",
    "    \"Perceptron\": Perceptron(max_iter=1000),\n",
    "    \"SVM lineal\": SVC(kernel=\"linear\"),\n",
    "    \"KNN (k=5)\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "for nombre, modelo in modelos.items():\n",
    "    modelo.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, modelo.predict(X_test))\n",
    "    print(f\"{modelo}: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a852391e",
   "metadata": {},
   "source": [
    "- Usa sepal length y petal width para entrenar una SVM lineal.\n",
    "- Dibuja la superficie de decisión y los vectores soporte. ¿Cuántos vectores soporte hay?\n",
    "- Repite con C=0.1, C=1 y C=10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf45836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"iris.csv\")\n",
    "df = df[df[\"class\"].isin([\"Iris-setosa\", \"Iris-versicolor\"])]  # Binario\n",
    "\n",
    "# Seleccionar dos variables\n",
    "X = df[[\"sepal length\", \"petal width\"]]\n",
    "y = LabelEncoder().fit_transform(df[\"class\"])  # 0 y 1\n",
    "\n",
    "# Normalizar y dividir\n",
    "X_scaled = MinMaxScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Función para graficar para cada valor de C\n",
    "def graficar_svm_lineal(C_val):\n",
    "    clf = SVC(kernel='linear', C=C_val)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Dibujar muestras\n",
    "    for clase in np.unique(y_train):\n",
    "        plt.scatter(X_train[y_train == clase, 0], X_train[y_train == clase, 1], label=f\"Clase {clase}\")\n",
    "\n",
    "    # Vectores soporte\n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], c='k', marker='*', label='Vectores soporte')\n",
    "\n",
    "    # Recta de decisión\n",
    "    m = -clf.coef_[0, 0] / clf.coef_[0, 1]\n",
    "    n = -clf.intercept_[0] / clf.coef_[0, 1]\n",
    "    x_vals = np.linspace(X_train[:, 0].min(), X_train[:, 0].max(), 100)\n",
    "    plt.plot(x_vals, m * x_vals + n, 'k-', label='Recta de decisión')\n",
    "\n",
    "    plt.xlabel(\"sepal length\")\n",
    "    plt.ylabel(\"petal width\")\n",
    "    plt.title(f\"SVM Lineal (C={C_val})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"C={C_val} -> Vectores soporte: {len(clf.support_)}\")\n",
    "\n",
    "# Ejecutar para los valores deseados de C\n",
    "for C in [0.1, 1, 10]:\n",
    "    graficar_svm_lineal(C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c56b44",
   "metadata": {},
   "source": [
    "\n",
    "Ejercicio 3: Multiclase con KNN\n",
    "- Prueba con k = 1, k = 3, k = 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [1, 3, 9]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Knn (k={k}) Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eada344",
   "metadata": {},
   "source": [
    "Vas a desarrollar una aplicación de visión por computador cuyo objetivo es determinar el género de una persona al entrar en un local. Para ello, trabajarás con la base de datos CelebA, la cual incluye, para cada imagen, el género de la persona y un conjunto de características visuales como formato, color, forma facial, entre otras.\n",
    "\n",
    "Estas características no se extraen directamente de las imágenes, sino que han sido preprocesadas previamente por otro equipo, utilizando un algoritmo basado en redes neuronales para generar representaciones numéricas de cada imagen.\n",
    "\n",
    "Tu tarea se centra en la parte de clasificación, es decir, en diseñar, entrenar y evaluar un modelo que, a partir de estas características extraídas, sea capaz de predecir correctamente el género de las personas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51abb9e",
   "metadata": {},
   "source": [
    "1.Leer el conjunto de entrenamiento y de test de los archivos CelebA-1K-train.csv\n",
    "y CelebA-1K-test.csv respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f062a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"CelebA-1K-train.csv\")\n",
    "test_df = pd.read_csv(\"CelebA-1K-test.csv\")\n",
    "\n",
    "X_train_vals = train_df.drop(columns=\"Gender\")\n",
    "y_train_vals = train_df[\"Gender\"]\n",
    "X_test_vals = test_df.drop(columns=\"Gender\")\n",
    "y_test_vals = test_df[\"Gender\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554818b",
   "metadata": {},
   "source": [
    "2.Entrenar un clasificador lineal biclásico utilizando el método de Regresión Logística\n",
    "y otro usando el perceptron obtener la tasa de acierto de los clasificadorres en las muestras del conjunto de test utilizar la función accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e0d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresión Logística\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_vals, y_train_vals)  # Se usa y_train_vals, no y_test_vals\n",
    "y_pred_log = log_reg.predict(X_test_vals)\n",
    "acc_log = accuracy_score(y_test_vals, y_pred_log)\n",
    "\n",
    "# Perceptrón\n",
    "percep = Perceptron(max_iter=1000)\n",
    "percep.fit(X_train_vals, y_train_vals)\n",
    "y_pred_percep = percep.predict(X_test_vals)\n",
    "acc_percep = accuracy_score(y_test_vals, y_pred_percep)  # Nombre distinto para no sobrescribir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2e304",
   "metadata": {},
   "source": [
    "3.Clasificar las imágenes que están en el archivo comprimido ImagenesParaClasificar.zip\n",
    "y cuyas características están en el conjunto de datos de test CelebA-1K-test.csv,\n",
    "indicando cuales de ellas las clasifica incorrectamente el clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded78f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def clasificar_imagenes(clf, df_test, carpeta_imagenes='ImagenesParaClasificar'):\n",
    "    \"\"\"\n",
    "    Clasifica las imágenes en una carpeta utilizando un clasificador ya entrenado\n",
    "    y un DataFrame con la información correspondiente.\n",
    "\n",
    "    Parámetros:\n",
    "    - clf: clasificador entrenado (por ejemplo, un modelo de sklearn)\n",
    "    - df_test: DataFrame con columnas ['Image_name', 'Genero', ...características]\n",
    "    - carpeta_imagenes: nombre de la carpeta con las imágenes a clasificar\n",
    "    \"\"\"\n",
    "    nombres = os.listdir(carpeta_imagenes)\n",
    "    print(f'Se encontraron {len(nombres)} imágenes para clasificar.\\n')\n",
    "\n",
    "    for nombre in nombres:\n",
    "        indice = df_test['Image_name'] == nombre\n",
    "        fila = df_test[indice]\n",
    "        \n",
    "        if fila.empty:\n",
    "            print(f'{nombre}: no se encontró en el DataFrame.\\n')\n",
    "            continue\n",
    "\n",
    "        genero = fila.values[0, 1]\n",
    "        X = fila.values[:, 2:]\n",
    "\n",
    "        genero_pred = clf.predict(X)\n",
    "        print(f'{nombre} → género etiquetado: {genero}')\n",
    "        print(f'{nombre} → género predicho:  {genero_pred[0]}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9359b81c",
   "metadata": {},
   "source": [
    "1. Leer el dataset de precios de casas, normaliza y dividelo en train y test. (80% - 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c8239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"PrecioCasas.csv\", sep = \",\")\n",
    "X = df.drop(columns=\"Precio\")\n",
    "y = df[\"Precio\"]\n",
    "\n",
    "scaled = MinMaxScaler()\n",
    "X_scaled = scaled.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ada07",
   "metadata": {},
   "source": [
    "2. Medir el tiempo de entrenamiento y test para los métodos `ball_tree`, `kd_tree` y `brute` del KNN usando el conjunto completo de características con jobs = 1.\n",
    "    - ¿Hay diferencias significativas en el tiempo de ejecución entre los métodos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1a8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Métodos a evaluar\n",
    "algorithms = ['ball_tree', 'kd_tree', 'brute']\n",
    "resultados = {}\n",
    "\n",
    "for algo in algorithms:\n",
    "    knn = KNeighborsClassifier(algorithm=algo, n_neighbors=5, n_jobs=1)\n",
    "\n",
    "    # Medir tiempo de entrenamiento\n",
    "    t0 = time.time()\n",
    "    knn.fit(X_train, y_train)\n",
    "    t_entrenamiento = time.time() - t0\n",
    "\n",
    "    # Medir tiempo de predicción\n",
    "    t1 = time.time()\n",
    "    y_pred = knn.predict(X_test)\n",
    "    t_test = time.time() - t1\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    resultados[algo] = {\n",
    "        'tiempo_entrenamiento': t_entrenamiento,\n",
    "        'tiempo_test': t_test,\n",
    "        'accuracy': acc\n",
    "    }\n",
    "\n",
    "resultados_df = pd.DataFrame(resultados).T\n",
    "print(resultados_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209737bd",
   "metadata": {},
   "source": [
    "### **Ejercicio: Impacto de la Normalización y del Parámetro de Regularización en SVM**\n",
    "\n",
    "En este ejercicio, se analizará cómo afectan la normalización de las variables de entrada y el parámetro de regularización `C` al rendimiento de un clasificador SVM.\n",
    "\n",
    "Utilizando los conjuntos de datos `precio_casas_clasificacion.csv` y `CelebA-2K.csv` con 80% train y 20% test, realiza las siguientes tareas:\n",
    "\n",
    "1. **Entrenamiento sin normalización:**\n",
    "   - Entrena un **clasificador lineal** y un **SVM con diferentes kernels** (por ejemplo: lineal, RBF, polinómico) usando las variables **sin normalizar**.\n",
    "   - Observa y comenta los resultados. Ten en cuenta que algunas variables, como en el caso del dataset de precios de casas, pueden tener rangos muy distintos.\n",
    "\n",
    "2. **Entrenamiento con normalización:**\n",
    "   - Aplica **dos técnicas de normalización** a las variables de entrada:\n",
    "     - **Escalado lineal (MinMaxScaler)**\n",
    "     - **Estandarización (StandardScaler)**\n",
    "   - Repite el entrenamiento con los mismos modelos (lineal y SVM con diferentes kernels) y compara los resultados con los obtenidos en el paso anterior.\n",
    "\n",
    "3. **Análisis del parámetro `C`:**\n",
    "   - Para cada configuración anterior (con y sin normalización), prueba diferentes valores del **parámetro de regularización `C`** (por ejemplo: `0.01`, `0.1`, `1`, `10`, `100`).\n",
    "   - Analiza cómo afecta este parámetro a la **exactitud del modelo** en el conjunto de prueba.\n",
    "\n",
    "4. **Compara con otros clasificadores el rendimento:**\n",
    "   - KNN\n",
    "   - Regresión logística\n",
    "   - Perceptrón\n",
    "\n",
    "4. **Conclusión:**\n",
    "   - Resume tus observaciones sobre cómo influyen la normalización y el parámetro `C` en el rendimiento de los modelos SVM.\n",
    "   - Comenta si hay diferencias significativas entre los dos datasets en cuanto a sensibilidad a la normalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b364509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "# Cargar datasets\n",
    "df_casas = pd.read_csv(\"precio_casas_clasificacion.csv\")\n",
    "X = df_casas.drop(columns=\"Precio\")\n",
    "y = df_casas[\"Precio\"]\n",
    "scaled = MinMaxScaler()\n",
    "X_scaled = scaled.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Logistic Regression: {acc*100:.4f}\")\n",
    "\n",
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred1 = svm.predict(X_test)\n",
    "acc1 = accuracy_score(y_test, y_pred1)\n",
    "print(f\"SVM: {acc1*100:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef546b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "# Cargar datasets\n",
    "df_casas = pd.read_csv(\"precio_casas_clasificacion.csv\")\n",
    "X = df_casas.drop(columns=\"Precio\")\n",
    "y = df_casas[\"Precio\"]\n",
    "scaled = StandardScaler()\n",
    "X_scaled = scaled.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Logistic Regression: {acc*100:.4f}\")\n",
    "\n",
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred1 = svm.predict(X_test)\n",
    "acc1 = accuracy_score(y_test, y_pred1)\n",
    "print(f\"SVM: {acc1*100:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd26ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "# Cargar datasets\n",
    "df_casas = pd.read_csv(\"precio_casas_clasificacion.csv\")\n",
    "X = df_casas.drop(columns=\"Precio\")\n",
    "y = df_casas[\"Precio\"]\n",
    "scaled = MinMaxScaler()\n",
    "X_scaled = scaled.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Logistic Regression: {acc*100:.4f}%\")\n",
    "\n",
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred1 = svm.predict(X_test)\n",
    "acc1 = accuracy_score(y_test, y_pred1)\n",
    "print(f\"SVM: {acc1*100:.4f}%\")\n",
    "\n",
    "percep = Perceptron(max_iter=1000)\n",
    "percep.fit(X_train, y_train)\n",
    "y_pred2 = percep.predict(X_test)\n",
    "acc2 = accuracy_score(y_test, y_pred2)\n",
    "print(f\"Perceptron: {acc2*100:.4f}%\")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred3 = knn.predict(X_test)\n",
    "acc3 = accuracy_score(y_test, y_pred3)\n",
    "print(f\"KNN: {acc3*100:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6601296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "# Cargar datasets\n",
    "df_celeb = pd.read_csv(\"CelebA-2K.csv\")\n",
    "X = df_celeb.drop(columns=[\"Image_name\", \"Gender\"])\n",
    "y = df_celeb[\"Gender\"]\n",
    "scaled = StandardScaler()\n",
    "X_scaled = scaled.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Logistic Regression: {acc*100:.4f}%\")\n",
    "\n",
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred1 = svm.predict(X_test)\n",
    "acc1 = accuracy_score(y_test, y_pred1)\n",
    "print(f\"SVM: {acc1*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f913bd3",
   "metadata": {},
   "source": [
    "La normalizacion te ayuda a que todas las colunnas tengan un valor equitativo, ya que puede que hayan columnas con valores muy grande por lo que el modelo podria darle más peso a esta, aunque no sea la más importante. En la regresión logistica, en este caso, si no se normaliza, el modelo no congerje, además ayuda a que el alogritmo llegue mas rapido y mejor a la solución optima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97156042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "# Cargar datasets\n",
    "df_celeb = pd.read_csv(\"CelebA-2K.csv\")\n",
    "X = df_celeb.drop(columns=[\"Image_name\", \"Gender\"])\n",
    "y = df_celeb[\"Gender\"]\n",
    "scaled = MinMaxScaler()\n",
    "X_scaled = scaled.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Logistic Regression: {acc*100:.4f}%\")\n",
    "\n",
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred1 = svm.predict(X_test)\n",
    "acc1 = accuracy_score(y_test, y_pred1)\n",
    "print(f\"SVM: {acc1*100:.4f}%\")\n",
    "\n",
    "percep = Perceptron(max_iter=1000)\n",
    "percep.fit(X_train, y_train)\n",
    "y_pred2 = percep.predict(X_test)\n",
    "acc2 = accuracy_score(y_test, y_pred2)\n",
    "print(f\"Perceptron: {acc2*100:.4f}%\")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred3 = knn.predict(X_test)\n",
    "acc3 = accuracy_score(y_test, y_pred)\n",
    "print(f\"KNN: {acc3*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c953cb5",
   "metadata": {},
   "source": [
    "La normalizacion te ayuda a que todas las colunnas tengan un valor equitativo, ya que puede que hayan columnas con valores muy grande por lo que el modelo podria darle más peso a esta, aunque no sea la más importante. En la regresión logistica, en este caso, si no se normaliza, el modelo no congerje, además ayuda a que el alogritmo llegue mas rapido y mejor a la solución optima. El archivo \"CelebA-2k.csv\" es mejor para cualquier modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f81ad17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regresion logica: 96.6667\n",
      "Perceptron: 83.3333\n",
      "SVM: 100.0000\n",
      "KNN: 100.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "\n",
    "df = pd.read_csv(\"iris.csv\", sep = \",\")\n",
    "X = df.drop(columns=\"class\")\n",
    "y = df[\"class\"]\n",
    "\n",
    "escaler = MinMaxScaler()\n",
    "X_scaled = escaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Regresion logica: {acc*100:.4f}\")\n",
    "\n",
    "percep = Perceptron()\n",
    "percep.fit(X_train, y_train)\n",
    "y_pred = percep.predict(X_test)\n",
    "acc2 = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Perceptron: {acc2*100:.4f}\")\n",
    "\n",
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"SVM: {acc*100:.4f}\")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"KNN: {acc*100:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
